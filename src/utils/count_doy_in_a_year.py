#!/usr/bin/env python3
"""
doy_statistics.py - ç»Ÿè®¡TIFFæ–‡ä»¶å¯¹åº”ä½ç½®çš„å“¨å…µ1/2æ•°æ®DOYæ•°é‡
åˆ›å»ºæ—¥æœŸï¼š2025-05-24
åŠŸèƒ½ï¼šå¹¶è¡ŒåŒ–å¤„ç†å¤šä¸ªTIFFæ–‡ä»¶ï¼Œç»Ÿè®¡2022å¹´å“¨å…µ1/2æ•°æ®çš„å”¯ä¸€DOYæ•°é‡ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
ä¼˜åŒ–ï¼šæ·»åŠ è¯·æ±‚é™æµã€æ”¹è¿›é”™è¯¯å¤„ç†ã€å‡å°‘APIå‹åŠ›
"""

import os
import sys
import time
import logging
import argparse
from pathlib import Path
from datetime import datetime, timedelta
import concurrent.futures
import threading
import random
import queue
from collections import defaultdict

import numpy as np
import pandas as pd
import rasterio
from rasterio.warp import transform_bounds
import pystac_client
import planetary_computer

# è®¾ç½®æ—¥å¿—
def setup_logging(debug=False):
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('doy_statistics.log', 'a', encoding='utf-8')
        ]
    )

# å…¨å±€å¸¸é‡
TIFF_DIR = "/scratch/zf281/global_map_1_degree_tiff"
OUTPUT_EXCEL = "/maps/zf281/btfm4rs/src/utils/doy_statistics_2022_1_degree_grid.xlsx"
YEAR = 2022
START_DATE = f"{YEAR}-01-01"
END_DATE = f"{YEAR}-12-31"

# é‡è¯•é…ç½® - æ›´ä¿å®ˆçš„è®¾ç½®
MAX_RETRIES = 8
RETRY_BACKOFF_FACTOR = 2.0
BASE_RETRY_DELAY = 3
MAX_RETRY_DELAY = 120

# å¹¶è¡Œé…ç½® - æ›´ä¿å®ˆçš„è®¾ç½®ä»¥é¿å…APIè¶…æ—¶
DEFAULT_MAX_WORKERS = 2  # å‡å°‘é»˜è®¤å¹¶å‘æ•°
DEFAULT_BATCH_SIZE = 4   # å‡å°‘æ‰¹å¤„ç†å¤§å°

# è¯·æ±‚é™æµé…ç½®
REQUEST_DELAY = 1.0  # æ¯ä¸ªè¯·æ±‚é—´éš”1ç§’
API_QUOTA_DELAY = 0.5  # APIé…é¢å»¶è¿Ÿ

# å…¨å±€é”å’Œé™æµå™¨
excel_lock = threading.Lock()
api_limiter = threading.Semaphore(2)  # æœ€å¤šåŒæ—¶2ä¸ªAPIè¯·æ±‚
request_times = queue.Queue()  # è®°å½•è¯·æ±‚æ—¶é—´ç”¨äºé™æµ

def rate_limit_request():
    """è¯·æ±‚é™æµå™¨ - ç¡®ä¿ä¸ä¼šè¿‡å¿«è¯·æ±‚API"""
    with api_limiter:
        current_time = time.time()
        
        # æ¸…ç†æ—§çš„è¯·æ±‚æ—¶é—´è®°å½•ï¼ˆè¶…è¿‡60ç§’çš„ï¼‰
        while not request_times.empty():
            try:
                old_time = request_times.get_nowait()
                if current_time - old_time > 60:
                    continue
                else:
                    request_times.put(old_time)
                    break
            except queue.Empty:
                break
        
        # è®°å½•å½“å‰è¯·æ±‚æ—¶é—´
        request_times.put(current_time)
        
        # æ·»åŠ è¯·æ±‚é—´éš”
        time.sleep(REQUEST_DELAY)

def get_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="ç»Ÿè®¡TIFFæ–‡ä»¶å¯¹åº”ä½ç½®çš„å“¨å…µ1/2æ•°æ®DOYæ•°é‡")
    parser.add_argument("--tiff_dir", default=TIFF_DIR, help="TIFFæ–‡ä»¶ç›®å½•è·¯å¾„")
    parser.add_argument("--output", default=OUTPUT_EXCEL, help="è¾“å‡ºExcelæ–‡ä»¶å")
    parser.add_argument("--year", type=int, default=YEAR, help="ç»Ÿè®¡å¹´ä»½")
    parser.add_argument("--max_workers", type=int, default=DEFAULT_MAX_WORKERS, help="æœ€å¤§å¹¶è¡Œæ•°")
    parser.add_argument("--batch_size", type=int, default=DEFAULT_BATCH_SIZE, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--request_delay", type=float, default=REQUEST_DELAY, help="è¯·æ±‚é—´éš”(ç§’)")
    return parser.parse_args()

def validate_bbox(bbox_ll, tile_id):
    """
    éªŒè¯è¾¹ç•Œæ¡†æ˜¯å¦æœ‰æ•ˆï¼Œæ£€æµ‹è·¨è¶Š180åº¦ç»çº¿çš„æƒ…å†µ
    bbox_llæ ¼å¼: [min_lon, min_lat, max_lon, max_lat]
    """
    min_lon, min_lat, max_lon, max_lat = bbox_ll
    
    # æ£€æŸ¥åŸºæœ¬çš„ç»çº¬åº¦èŒƒå›´
    if not (-180 <= min_lon <= 180 and -180 <= max_lon <= 180):
        logging.warning(f"{tile_id}: ç»åº¦è¶…å‡ºæœ‰æ•ˆèŒƒå›´ [-180, 180]: min_lon={min_lon}, max_lon={max_lon}")
        return False
    
    if not (-90 <= min_lat <= 90 and -90 <= max_lat <= 90):
        logging.warning(f"{tile_id}: çº¬åº¦è¶…å‡ºæœ‰æ•ˆèŒƒå›´ [-90, 90]: min_lat={min_lat}, max_lat={max_lat}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦è·¨è¶Š180åº¦ç»çº¿ï¼ˆå›½é™…æ—¥æœŸå˜æ›´çº¿ï¼‰
    if min_lon > max_lon:
        logging.warning(f"{tile_id}: âš ï¸ è¾¹ç•Œæ¡†è·¨è¶Š180åº¦ç»çº¿ï¼ˆå›½é™…æ—¥æœŸå˜æ›´çº¿ï¼‰ï¼ŒSTAC APIæ— æ³•å¤„ç†")
        logging.warning(f"{tile_id}: bbox=[{min_lon:.3f}, {min_lat:.3f}, {max_lon:.3f}, {max_lat:.3f}]")
        logging.warning(f"{tile_id}: è¿™é€šå¸¸å‘ç”Ÿåœ¨æ¥è¿‘å¤ªå¹³æ´‹æ—¥æœŸå˜æ›´çº¿çš„åŒºåŸŸï¼Œå°†è·³è¿‡æ­¤tile")
        return False
    
    # æ£€æŸ¥çº¬åº¦æ˜¯å¦æ­£ç¡®
    if min_lat > max_lat:
        logging.warning(f"{tile_id}: æœ€å°çº¬åº¦å¤§äºæœ€å¤§çº¬åº¦: min_lat={min_lat}, max_lat={max_lat}")
        return False
    
    return True
def load_tiff_bounds(tiff_path):
    """
    åŠ è½½TIFFæ–‡ä»¶çš„è¾¹ç•Œæ¡†ä¿¡æ¯
    è¿”å›ï¼š(bbox_proj, bbox_ll, crs)
    """
    try:
        with rasterio.open(tiff_path) as src:
            bbox_proj = src.bounds
            crs = src.crs
            # è½¬æ¢ä¸ºWGS84ç»çº¬åº¦åæ ‡
            bbox_ll = transform_bounds(crs, "EPSG:4326", *bbox_proj, densify_pts=21)
            return bbox_proj, bbox_ll, crs
    except Exception as e:
        logging.error(f"è¯»å–TIFFæ–‡ä»¶ {tiff_path} å¤±è´¥: {e}")
        return None, None, None

def search_sentinel_items_with_fallback(bbox_ll, date_range, collection, query_params=None, max_retries=MAX_RETRIES, tile_id="unknown"):
    """
    æœç´¢å“¨å…µæ•°æ®ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œfallbackç­–ç•¥
    """
    retry_count = 0
    retry_delay = BASE_RETRY_DELAY
    
    while retry_count <= max_retries:
        try:
            # åº”ç”¨è¯·æ±‚é™æµ
            rate_limit_request()
            
            # åˆ›å»ºSTACå®¢æˆ·ç«¯
            cat = pystac_client.Client.open(
                "https://planetarycomputer.microsoft.com/api/stac/v1",
                modifier=planetary_computer.sign_inplace
            )
            
            # æ„å»ºæŸ¥è¯¢å‚æ•°
            search_params = {
                "collections": [collection],
                "bbox": bbox_ll,
                "datetime": date_range,
                "limit": 999  # è®¾ç½®ä¸º999é¿å…è¶…è¿‡1000é™åˆ¶
            }
            
            # æ·»åŠ é¢å¤–æŸ¥è¯¢å‚æ•°
            if query_params:
                search_params.update(query_params)
            
            logging.debug(f"{tile_id}: æœç´¢å‚æ•° {collection} - bbox: {bbox_ll}, datetime: {date_range}")
            if query_params and 'query' in query_params:
                logging.debug(f"{tile_id}: æŸ¥è¯¢è¿‡æ»¤å™¨: {query_params['query']}")
            
            # æ‰§è¡Œæœç´¢
            q = cat.search(**search_params)
            
            # ä½¿ç”¨items()è€Œä¸æ˜¯get_items()æ¥é¿å…FutureWarning
            try:
                items = list(q.items())
            except AttributeError:
                # å¦‚æœitems()æ–¹æ³•ä¸å­˜åœ¨ï¼Œä½¿ç”¨get_items()
                items = list(q.get_items())
            
            logging.info(f"{tile_id}: åŸå§‹æœç´¢åˆ° {len(items)} ä¸ªitems ({collection})")
            
            return items
            
        except Exception as e:
            error_msg = str(e).lower()
            retry_count += 1
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯
            is_timeout_error = any(keyword in error_msg for keyword in [
                'timeout', 'exceeded', 'maximum allowed time', 'timed out'
            ])
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œé”™è¯¯
            is_network_error = any(keyword in error_msg for keyword in [
                'connection', 'network', 'unreachable', 'refused'
            ])
            
            if retry_count > max_retries:
                logging.error(f"æœç´¢ {collection} æ•°æ®å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {e}")
                return []
            
            # æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´é‡è¯•ç­–ç•¥
            if is_timeout_error:
                # è¶…æ—¶é”™è¯¯ - ä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿ
                retry_delay = min(MAX_RETRY_DELAY, retry_delay * RETRY_BACKOFF_FACTOR * 1.5)
                logging.warning(f"æœç´¢ {collection} è¶…æ—¶ (å°è¯• {retry_count}/{max_retries+1}): {e}")
            elif is_network_error:
                # ç½‘ç»œé”™è¯¯ - ä¸­ç­‰å»¶è¿Ÿ
                retry_delay = min(MAX_RETRY_DELAY, retry_delay * RETRY_BACKOFF_FACTOR)
                logging.warning(f"æœç´¢ {collection} ç½‘ç»œé”™è¯¯ (å°è¯• {retry_count}/{max_retries+1}): {e}")
            else:
                # å…¶ä»–é”™è¯¯ - æ ‡å‡†å»¶è¿Ÿ
                retry_delay = min(MAX_RETRY_DELAY, retry_delay * RETRY_BACKOFF_FACTOR)
                logging.warning(f"æœç´¢ {collection} å¤±è´¥ (å°è¯• {retry_count}/{max_retries+1}): {e}")
            
            # æ·»åŠ éšæœºæŠ–åŠ¨
            jitter = random.uniform(0.8, 1.2)
            actual_delay = retry_delay * jitter
            
            logging.info(f"{actual_delay:.1f}ç§’åé‡è¯•...")
            time.sleep(actual_delay)
    
    return []

def extract_unique_doys(items, tile_id="unknown", collection="unknown", target_year=None):
    """
    ä»itemsä¸­æå–å”¯ä¸€çš„DOYï¼ˆDay of Yearï¼‰ï¼Œæ·»åŠ è¯¦ç»†æ—¥å¿—å’Œå¹´ä»½éªŒè¯
    """
    doys = set()
    valid_items = 0
    invalid_date_count = 0
    wrong_year_count = 0
    date_examples = []
    
    logging.info(f"{tile_id}: å¼€å§‹å¤„ç† {len(items)} ä¸ª {collection} itemsï¼Œç›®æ ‡å¹´ä»½: {target_year}")
    
    for i, item in enumerate(items):
        try:
            # è·å–æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
            datetime_str = item.properties.get("datetime", "")
            if not datetime_str:
                invalid_date_count += 1
                continue
                
            # è§£ææ—¥æœŸ
            dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
            
            # æ£€æŸ¥å¹´ä»½æ˜¯å¦åŒ¹é…ç›®æ ‡å¹´ä»½
            if target_year and dt.year != target_year:
                wrong_year_count += 1
                if wrong_year_count <= 3:  # åªè®°å½•å‰3ä¸ªé”™è¯¯å¹´ä»½çš„ä¾‹å­
                    logging.warning(f"{tile_id}: å‘ç°éç›®æ ‡å¹´ä»½æ•°æ® - æœŸæœ›{target_year}ï¼Œå®é™…{dt.year} ({datetime_str})")
                continue
            
            # è®¡ç®—DOY
            doy = dt.timetuple().tm_yday
            doys.add(doy)
            valid_items += 1
            
            # æ”¶é›†å‰5ä¸ªæ—¥æœŸä½œä¸ºä¾‹å­
            if len(date_examples) < 5:
                date_examples.append(f"{dt.strftime('%Y-%m-%d')}(DOY:{doy})")
                
        except Exception as e:
            invalid_date_count += 1
            if invalid_date_count <= 3:  # åªè®°å½•å‰3ä¸ªè§£æé”™è¯¯
                logging.warning(f"{tile_id}: è§£ææ—¥æœŸå¤±è´¥: {datetime_str}, é”™è¯¯: {e}")
            continue
    
    unique_doy_count = len(doys)
    
    # è¯¦ç»†æ—¥å¿—
    logging.info(f"{tile_id}: {collection} å¤„ç†ç»“æœ:")
    logging.info(f"{tile_id}:   - æ€»items: {len(items)}")
    logging.info(f"{tile_id}:   - æœ‰æ•ˆitems: {valid_items}")
    logging.info(f"{tile_id}:   - æ—¥æœŸè§£æå¤±è´¥: {invalid_date_count}")
    logging.info(f"{tile_id}:   - å¹´ä»½ä¸åŒ¹é…: {wrong_year_count}")
    logging.info(f"{tile_id}:   - å”¯ä¸€DOYæ•°é‡: {unique_doy_count}")
    logging.info(f"{tile_id}:   - æ—¥æœŸç¤ºä¾‹: {', '.join(date_examples)}")
    
    # å¦‚æœDOYæ•°é‡å¼‚å¸¸ï¼ˆ>366ï¼‰ï¼Œè¾“å‡ºæ›´è¯¦ç»†ä¿¡æ¯
    if unique_doy_count > 366:
        logging.error(f"{tile_id}: âš ï¸ å¼‚å¸¸ï¼DOYæ•°é‡ {unique_doy_count} è¶…è¿‡ä¸€å¹´æœ€å¤§å¤©æ•°ï¼")
        logging.error(f"{tile_id}: DOYåˆ—è¡¨ï¼ˆå‰20ä¸ªï¼‰: {sorted(list(doys))[:20]}")
        
        # æŒ‰å¹´ä»½åˆ†ç»„æ˜¾ç¤ºç»Ÿè®¡
        year_stats = defaultdict(int)
        for item in items[:50]:  # åªæ£€æŸ¥å‰50ä¸ªé¿å…å¤ªå¤šæ—¥å¿—
            try:
                datetime_str = item.properties.get("datetime", "")
                if datetime_str:
                    dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
                    year_stats[dt.year] += 1
            except:
                continue
        logging.error(f"{tile_id}: å¹´ä»½åˆ†å¸ƒç»Ÿè®¡: {dict(year_stats)}")
    
    return unique_doy_count

def process_single_tiff(tiff_path, year, request_delay=REQUEST_DELAY):
    """
    å¤„ç†å•ä¸ªTIFFæ–‡ä»¶ï¼Œè¿”å›ç»Ÿè®¡ç»“æœ
    """
    tiff_path = Path(tiff_path)
    tile_id = tiff_path.stem
    
    logging.info(f"å¼€å§‹å¤„ç† TIFF: {tile_id}")
    
    # è¯»å–TIFFè¾¹ç•Œ
    bbox_proj, bbox_ll, crs = load_tiff_bounds(tiff_path)
    if bbox_ll is None:
        logging.error(f"æ— æ³•è¯»å– {tile_id} çš„è¾¹ç•Œä¿¡æ¯")
        return tile_id, 0, 0, 0
    
    # éªŒè¯è¾¹ç•Œæ¡†æ˜¯å¦æœ‰æ•ˆï¼ˆæ£€æŸ¥è·¨è¶Š180åº¦ç»çº¿ç­‰é—®é¢˜ï¼‰
    if not validate_bbox(bbox_ll, tile_id):
        logging.warning(f"âš ï¸ {tile_id}: è¾¹ç•Œæ¡†æ— æ•ˆï¼Œè·³è¿‡å¤„ç†")
        return tile_id, -1, -1, -1  # ä½¿ç”¨-1æ ‡è®°ä¸ºè·³è¿‡çš„tile
    
    # ç¡®ä¿æ—¥æœŸèŒƒå›´ä¸¥æ ¼é™åˆ¶åœ¨æŒ‡å®šå¹´ä»½
    date_range = f"{year}-01-01T00:00:00Z/{year}-12-31T23:59:59Z"
    logging.info(f"{tile_id}: ä½¿ç”¨ä¸¥æ ¼æ—¥æœŸèŒƒå›´: {date_range}")
    logging.info(f"{tile_id}: æœ‰æ•ˆè¾¹ç•Œæ¡†: [{bbox_ll[0]:.3f}, {bbox_ll[1]:.3f}, {bbox_ll[2]:.3f}, {bbox_ll[3]:.3f}]")
    
    # åˆå§‹åŒ–ç»“æœ
    s2_doy_count = 0
    s1_asc_doy_count = 0
    s1_desc_doy_count = 0
    
    try:
        # æœç´¢Sentinel-2æ•°æ®ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„æ­£ç¡®æ–¹å¼ï¼‰
        logging.info(f"{tile_id}: æœç´¢Sentinel-2æ•°æ®...")
        s2_query_params = {
            "query": {
                "eo:cloud_cover": {"lt": 90}  # äº‘é‡é™åˆ¶æ”¹ä¸º90%
            }
        }
        s2_items_raw = search_sentinel_items_with_fallback(
            bbox_ll, 
            date_range, 
            "sentinel-2-l2a",
            query_params=s2_query_params,
            tile_id=tile_id
        )
        
        # è¿‡æ»¤åŒ…å«MSIæ³¢æ®µçš„itemsï¼ˆä¾‹å¦‚B04çº¢è‰²æ³¢æ®µï¼Œæ‰€æœ‰S2 L2Aéƒ½åº”è¯¥æœ‰ï¼‰
        s2_items = [item for item in s2_items_raw if 'B04' in item.assets]
        logging.info(f"{tile_id}: S2è¿‡æ»¤åä¿ç•™ {len(s2_items)}/{len(s2_items_raw)} ä¸ªåŒ…å«B04(MSI)æ³¢æ®µçš„items")
        
        s2_doy_count = extract_unique_doys(s2_items, tile_id, "Sentinel-2", year)
        logging.info(f"{tile_id}: âœ“ S2 æœ€ç»ˆDOYæ•°é‡ = {s2_doy_count}")
        
        # æ·»åŠ é¢å¤–å»¶è¿Ÿ
        time.sleep(request_delay)
        
        # æœç´¢Sentinel-1å‡è½¨æ•°æ®
        logging.info(f"{tile_id}: æœç´¢Sentinel-1å‡è½¨æ•°æ®...")
        s1_asc_items = search_sentinel_items_with_fallback(
            bbox_ll,
            date_range,
            "sentinel-1-rtc",
            query_params={"query": {"sat:orbit_state": {"eq": "ascending"}}},
            tile_id=tile_id
        )
        s1_asc_doy_count = extract_unique_doys(s1_asc_items, tile_id, "S1-ASC", year)
        logging.info(f"{tile_id}: âœ“ S1å‡è½¨ æœ€ç»ˆDOYæ•°é‡ = {s1_asc_doy_count}")
        
        # æ·»åŠ é¢å¤–å»¶è¿Ÿ
        time.sleep(request_delay)
        
        # æœç´¢Sentinel-1é™è½¨æ•°æ®
        logging.info(f"{tile_id}: æœç´¢Sentinel-1é™è½¨æ•°æ®...")
        s1_desc_items = search_sentinel_items_with_fallback(
            bbox_ll,
            date_range,
            "sentinel-1-rtc",
            query_params={"query": {"sat:orbit_state": {"eq": "descending"}}},
            tile_id=tile_id
        )
        s1_desc_doy_count = extract_unique_doys(s1_desc_items, tile_id, "S1-DESC", year)
        logging.info(f"{tile_id}: âœ“ S1é™è½¨ æœ€ç»ˆDOYæ•°é‡ = {s1_desc_doy_count}")
        
    except Exception as e:
        logging.error(f"å¤„ç† {tile_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    
    logging.info(f"ğŸ¯ å®Œæˆ {tile_id}: S2={s2_doy_count}, S1_ASC={s1_asc_doy_count}, S1_DESC={s1_desc_doy_count}")
    return tile_id, s2_doy_count, s1_asc_doy_count, s1_desc_doy_count

def load_existing_results(excel_path):
    """
    åŠ è½½ç°æœ‰çš„Excelç»“æœï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
    """
    # ç¡®ä¿è¾“å‡ºè·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
    excel_path = Path(excel_path).resolve()
    
    if not excel_path.exists():
        # åˆ›å»ºæ–°çš„DataFrame
        df = pd.DataFrame(columns=['Tile_id', 's2_doy', 's1_ascending_doy', 's1_descending_doy'])
        logging.info(f"åˆ›å»ºæ–°çš„ç»“æœæ–‡ä»¶: {excel_path}")
        return df, excel_path
    
    try:
        df = pd.read_excel(excel_path)
        logging.info(f"åŠ è½½äº†ç°æœ‰ç»“æœæ–‡ä»¶: {excel_path}ï¼ŒåŒ…å« {len(df)} æ¡è®°å½•")
        return df, excel_path
    except Exception as e:
        logging.error(f"è¯»å–ç°æœ‰ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
        # è¿”å›ç©ºDataFrame
        return pd.DataFrame(columns=['Tile_id', 's2_doy', 's1_ascending_doy', 's1_descending_doy']), excel_path

def save_results_to_excel(df, excel_path):
    """
    ä¿å­˜ç»“æœåˆ°Excelæ–‡ä»¶
    """
    try:
        with excel_lock:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            excel_path.parent.mkdir(parents=True, exist_ok=True)
            df.to_excel(excel_path, index=False)
        logging.info(f"ç»“æœå·²ä¿å­˜åˆ° {excel_path} ({len(df)} æ¡è®°å½•)")
    except Exception as e:
        logging.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")

def is_record_complete(row):
    """
    æ£€æŸ¥è®°å½•æ˜¯å¦å®Œæ•´ï¼ˆæ‰€æœ‰åˆ—éƒ½æœ‰æœ‰æ•ˆå€¼ï¼‰
    -1è¡¨ç¤ºè·³è¿‡çš„tileï¼ˆä¾‹å¦‚è¾¹ç•Œæ¡†è·¨è¶Š180åº¦ç»çº¿ï¼‰
    """
    return (
        pd.notna(row.get('s2_doy', np.nan)) and 
        pd.notna(row.get('s1_ascending_doy', np.nan)) and 
        pd.notna(row.get('s1_descending_doy', np.nan)) and
        (
            # æ­£å¸¸æƒ…å†µï¼šæ‰€æœ‰å€¼éƒ½>=0
            (row.get('s2_doy', -2) >= 0 and
             row.get('s1_ascending_doy', -2) >= 0 and
             row.get('s1_descending_doy', -2) >= 0) or
            # è·³è¿‡æƒ…å†µï¼šæ‰€æœ‰å€¼éƒ½æ˜¯-1
            (row.get('s2_doy', -2) == -1 and
             row.get('s1_ascending_doy', -2) == -1 and
             row.get('s1_descending_doy', -2) == -1)
        )
    )

def get_tiff_files(tiff_dir):
    """
    è·å–ç›®å½•ä¸‹æ‰€æœ‰TIFFæ–‡ä»¶
    """
    tiff_dir = Path(tiff_dir)
    if not tiff_dir.exists():
        logging.error(f"TIFFç›®å½•ä¸å­˜åœ¨: {tiff_dir}")
        return []
    
    tiff_files = list(tiff_dir.glob("*.tiff")) + list(tiff_dir.glob("*.tif"))
    logging.info(f"æ‰¾åˆ° {len(tiff_files)} ä¸ªTIFFæ–‡ä»¶")
    return tiff_files

def update_dataframe_with_result(df, tile_id, s2_doy, s1_asc_doy, s1_desc_doy):
    """
    æ›´æ–°DataFrameä¸­çš„ç»“æœ
    """
    # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥tile_idçš„è®°å½•
    mask = df['Tile_id'] == tile_id
    if mask.any():
        # æ›´æ–°ç°æœ‰è®°å½•
        df.loc[mask, 's2_doy'] = s2_doy
        df.loc[mask, 's1_ascending_doy'] = s1_asc_doy
        df.loc[mask, 's1_descending_doy'] = s1_desc_doy
    else:
        # æ·»åŠ æ–°è®°å½•
        new_row = {
            'Tile_id': tile_id,
            's2_doy': s2_doy,
            's1_ascending_doy': s1_asc_doy,
            's1_descending_doy': s1_desc_doy
        }
        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
    
    return df

def main():
    """ä¸»å‡½æ•°"""
    args = get_args()
    setup_logging(args.debug)
    
    # æ›´æ–°å…¨å±€è¯·æ±‚å»¶è¿Ÿ
    global REQUEST_DELAY
    REQUEST_DELAY = args.request_delay
    
    logging.info("="*60)
    logging.info("å¼€å§‹DOYç»Ÿè®¡ä»»åŠ¡")
    logging.info(f"TIFFç›®å½•: {args.tiff_dir}")
    logging.info(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    logging.info(f"ç»Ÿè®¡å¹´ä»½: {args.year}")
    logging.info(f"æœ€å¤§å¹¶è¡Œæ•°: {args.max_workers}")
    logging.info(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    logging.info(f"è¯·æ±‚é—´éš”: {args.request_delay}ç§’")
    logging.info("="*60)
    
    # è·å–æ‰€æœ‰TIFFæ–‡ä»¶
    tiff_files = get_tiff_files(args.tiff_dir)
    if not tiff_files:
        logging.error("æœªæ‰¾åˆ°TIFFæ–‡ä»¶ï¼Œé€€å‡º")
        return
    
    # åŠ è½½ç°æœ‰ç»“æœ
    df, excel_path = load_existing_results(args.output)
    
    # ç­›é€‰å‡ºéœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼ˆè·³è¿‡å·²å®Œæˆçš„ï¼‰
    pending_files = []
    for tiff_file in tiff_files:
        tile_id = tiff_file.stem
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å®Œæ•´è®°å½•
        existing_mask = df['Tile_id'] == tile_id
        if existing_mask.any():
            existing_row = df[existing_mask].iloc[0]
            if is_record_complete(existing_row):
                if existing_row.get('s2_doy', 0) == -1:
                    logging.debug(f"è·³è¿‡å·²æ ‡è®°ä¸ºæ— æ•ˆçš„tile: {tile_id}")
                else:
                    logging.debug(f"è·³è¿‡å·²å®Œæˆçš„tile: {tile_id}")
                continue
        
        pending_files.append(tiff_file)
    
    logging.info(f"éœ€è¦å¤„ç†çš„æ–‡ä»¶æ•°é‡: {len(pending_files)} / {len(tiff_files)}")
    
    if not pending_files:
        logging.info("æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆ")
        return
    
    # å¼€å§‹å¤„ç†
    start_time = time.time()
    processed_count = 0
    
    # ä½¿ç”¨æ›´ä¿å®ˆçš„çº¿ç¨‹æ± è®¾ç½®
    with concurrent.futures.ThreadPoolExecutor(max_workers=args.max_workers) as executor:
        # åˆ†æ‰¹æäº¤ä»»åŠ¡
        for batch_start in range(0, len(pending_files), args.batch_size):
            batch_end = min(batch_start + args.batch_size, len(pending_files))
            batch_files = pending_files[batch_start:batch_end]
            
            logging.info(f"å¤„ç†æ‰¹æ¬¡ {batch_start//args.batch_size + 1}: {len(batch_files)} ä¸ªæ–‡ä»¶")
            
            # æäº¤å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
            future_to_file = {
                executor.submit(process_single_tiff, tiff_file, args.year, args.request_delay): tiff_file 
                for tiff_file in batch_files
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in concurrent.futures.as_completed(future_to_file):
                tiff_file = future_to_file[future]
                try:
                    tile_id, s2_doy, s1_asc_doy, s1_desc_doy = future.result()
                    
                    # æ›´æ–°DataFrame
                    df = update_dataframe_with_result(df, tile_id, s2_doy, s1_asc_doy, s1_desc_doy)
                    
                    processed_count += 1
                    
                    # è®°å½•å¤„ç†ç»“æœç±»å‹
                    if s2_doy == -1 and s1_asc_doy == -1 and s1_desc_doy == -1:
                        logging.info(f"ğŸ“‹ {tile_id}: å·²è·³è¿‡ï¼ˆè¾¹ç•Œæ¡†é—®é¢˜ï¼‰")
                    else:
                        logging.info(f"âœ… {tile_id}: å¤„ç†å®Œæˆ (S2={s2_doy}, S1_ASC={s1_asc_doy}, S1_DESC={s1_desc_doy})")
                    
                    # æ¯å¤„ç†å®Œ3ä¸ªæ–‡ä»¶å°±ä¿å­˜ä¸€æ¬¡ï¼ˆå‡å°‘ä¿å­˜é¢‘ç‡ï¼‰
                    if processed_count % 3 == 0:
                        save_results_to_excel(df, excel_path)
                        elapsed_time = time.time() - start_time
                        avg_time = elapsed_time / processed_count
                        remaining_files = len(pending_files) - processed_count
                        eta = remaining_files * avg_time
                        
                        logging.info(f"è¿›åº¦: {processed_count}/{len(pending_files)} "
                                   f"({processed_count/len(pending_files)*100:.1f}%), "
                                   f"å¹³å‡ç”¨æ—¶: {avg_time:.1f}s/æ–‡ä»¶, "
                                   f"é¢„è®¡å‰©ä½™æ—¶é—´: {eta/60:.1f}åˆ†é’Ÿ")
                    
                except Exception as e:
                    logging.error(f"å¤„ç†æ–‡ä»¶ {tiff_file} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            
            # æ‰¹æ¬¡å®Œæˆåä¼‘æ¯æ›´é•¿æ—¶é—´ï¼Œå‡å°‘APIå‹åŠ›
            if batch_end < len(pending_files):
                batch_delay = min(30, args.batch_size * 2)  # æ ¹æ®æ‰¹æ¬¡å¤§å°è°ƒæ•´å»¶è¿Ÿ
                logging.info(f"æ‰¹æ¬¡å®Œæˆï¼Œä¼‘æ¯ {batch_delay} ç§’...")
                time.sleep(batch_delay)
    
    # æœ€ç»ˆä¿å­˜
    save_results_to_excel(df, excel_path)
    
    # ç»Ÿè®¡å„ç§å¤„ç†ç»“æœ
    skipped_count = len(df[df['s2_doy'] == -1])  # è·³è¿‡çš„tileæ•°é‡
    valid_count = len(df[df['s2_doy'] >= 0])     # æ­£å¸¸å¤„ç†çš„tileæ•°é‡
    
    # ç»Ÿè®¡æ€»ç»“
    total_time = time.time() - start_time
    logging.info("="*60)
    logging.info("DOYç»Ÿè®¡ä»»åŠ¡å®Œæˆ")
    logging.info(f"å¤„ç†æ–‡ä»¶æ•°: {processed_count}")
    logging.info(f"  - æ­£å¸¸å¤„ç†: {valid_count}")
    logging.info(f"  - è·³è¿‡å¤„ç†: {skipped_count} (è¾¹ç•Œæ¡†é—®é¢˜)")
    logging.info(f"æ€»ç”¨æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    if processed_count > 0:
        logging.info(f"å¹³å‡ç”¨æ—¶: {total_time/processed_count:.1f} ç§’/æ–‡ä»¶")
    logging.info(f"ç»“æœå·²ä¿å­˜åˆ°: {excel_path}")
    logging.info("="*60)

if __name__ == "__main__":
    main()